{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73494cc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): | DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): repo.anaconda.com:443\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): repo.anaconda.com:443\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): repo.anaconda.com:443\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): repo.anaconda.com:443\n",
      "DEBUG:urllib3.connectionpool:https://repo.anaconda.com:443 \"GET /pkgs/r/osx-arm64/current_repodata.json HTTP/1.1\" 304 0\n",
      "DEBUG:urllib3.connectionpool:https://repo.anaconda.com:443 \"GET /pkgs/main/noarch/current_repodata.json HTTP/1.1\" 304 0\n",
      "/ DEBUG:urllib3.connectionpool:https://repo.anaconda.com:443 \"GET /pkgs/r/noarch/current_repodata.json HTTP/1.1\" 304 0\n",
      "DEBUG:urllib3.connectionpool:https://repo.anaconda.com:443 \"GET /pkgs/main/osx-arm64/current_repodata.json HTTP/1.1\" 304 0\n",
      "done\n",
      "Solving environment: unsuccessful initial attempt using frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): / DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): repo.anaconda.com:443\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): repo.anaconda.com:443\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): repo.anaconda.com:443\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): repo.anaconda.com:443\n",
      "- DEBUG:urllib3.connectionpool:https://repo.anaconda.com:443 \"GET /pkgs/r/noarch/repodata.json HTTP/1.1\" 304 0\n",
      "DEBUG:urllib3.connectionpool:https://repo.anaconda.com:443 \"GET /pkgs/main/osx-arm64/repodata.json HTTP/1.1\" 304 0\n",
      "DEBUG:urllib3.connectionpool:https://repo.anaconda.com:443 \"GET /pkgs/r/osx-arm64/repodata.json HTTP/1.1\" 304 0\n",
      "\\ DEBUG:urllib3.connectionpool:https://repo.anaconda.com:443 \"GET /pkgs/main/noarch/repodata.json HTTP/1.1\" 304 0\n",
      "done\n",
      "Solving environment: unsuccessful initial attempt using frozen solve. Retrying with flexible solve.\n",
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - time\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - https://repo.anaconda.com/pkgs/main/osx-arm64\n",
      "  - https://repo.anaconda.com/pkgs/main/noarch\n",
      "  - https://repo.anaconda.com/pkgs/r/osx-arm64\n",
      "  - https://repo.anaconda.com/pkgs/r/noarch\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "712504e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jieba in ./anaconda3/lib/python3.11/site-packages (0.42.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install jieba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04360a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in ./anaconda3/lib/python3.11/site-packages (3.0.10)\n",
      "Requirement already satisfied: et_xmlfile in ./anaconda3/lib/python3.11/site-packages (from openpyxl) (1.1.0)\n",
      "/Users/xuejiashun\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "!pip install openpyxl\n",
    "import os\n",
    "print(os.getcwd())  # 获取当前工作目录\n",
    "import os\n",
    "notebook_dir = os.getcwd()  # 获取 Jupyter Notebook 的当前工作目录\n",
    "excel_file = os.path.join(notebook_dir, '情感词汇本体.xlsx')  # 拼接 Excel 文件的路径\n",
    "import pandas as pd\n",
    "df = pd.read_excel(excel_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0505186e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          微博内容  Unnamed: 1\n",
      "0  :#花西子跌出天猫双十一彩妆预售榜前20#网友的眼光都是雪亮的！花西子还是不够努力啊！         NaN\n",
      "1               :@公子小友：想一想是不是品牌不够努力，反思一下营销努力了吗         NaN\n",
      "2                         :@花西子Florasis 有没有反思？         NaN\n",
      "3                             :ta家包装让我很难有购买的欲望         NaN\n",
      "4                                           :啊         NaN\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import pandas as pd\n",
    "\n",
    "#获取数据集\n",
    "f = open('花西子跌出榜单前10评论.csv',encoding='utf-8')\n",
    "weibo_df = pd.read_csv(f)\n",
    "print(weibo_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15a8b4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    词语  词性种类  词义数  词义序号 情感分类  强度  极性 辅助情感分类  强度.1  极性.1 Unnamed: 10  \\\n",
      "0   脏乱   adj  1.0   1.0   NN   7   2    NaN   NaN   NaN         NaN   \n",
      "1   糟报   adj  1.0   1.0   NN   5   2    NaN   NaN   NaN         NaN   \n",
      "2   早衰   adj  1.0   1.0   NE   5   2    NaN   NaN   NaN         NaN   \n",
      "3   责备  verb  1.0   1.0   NN   5   2    NaN   NaN   NaN         NaN   \n",
      "4   贼眼  noun  1.0   1.0   NN   5   2    NaN   NaN   NaN         NaN   \n",
      "5   战祸  noun  1.0   1.0   ND   5   2     NC   5.0   2.0         NaN   \n",
      "6   招灾   adj  1.0   1.0   NN   5   2    NaN   NaN   NaN         NaN   \n",
      "7   折辱  noun  1.0   1.0   NE   5   2     NN   5.0   2.0         NaN   \n",
      "8  中山狼  noun  1.0   1.0   NN   5   2    NaN   NaN   NaN         NaN   \n",
      "9   清峻   adj  1.0   1.0   PH   5   0    NaN   NaN   NaN         NaN   \n",
      "\n",
      "   Unnamed: 11  \n",
      "0          NaN  \n",
      "1          NaN  \n",
      "2          NaN  \n",
      "3          NaN  \n",
      "4          NaN  \n",
      "5          NaN  \n",
      "6          NaN  \n",
      "7          NaN  \n",
      "8          NaN  \n",
      "9          NaN  \n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------情感词典读取-------------------------------\n",
    "#注意：\n",
    "#1.词典中怒的标记(NA)识别不出被当作空值,情感分类列中的NA都给替换成NAU\n",
    "#2.大连理工词典中有情感分类的辅助标注(有NA),故把情感分类列改好再替换原词典中\n",
    "\n",
    "# 扩展前的词典\n",
    "df = pd.read_excel('情感词汇本体.xlsx')\n",
    "print(df.head(10))\n",
    "\n",
    "df = df[['词语', '词性种类', '词义数', '词义序号', '情感分类', '强度', '极性']]\n",
    "df.head()\n",
    "\n",
    "# coding: utf-8\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe166a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "情绪词语列表整理完成\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#-------------------------------------七种情绪的运用-------------------------------\n",
    "Happy = []\n",
    "Good = []\n",
    "Surprise = []\n",
    "Anger = []\n",
    "Sad = []\n",
    "Fear = []\n",
    "Disgust = []\n",
    "\n",
    "#df.iterrows()功能是迭代遍历每一行\n",
    "for idx, row in df.iterrows():\n",
    "    if row['情感分类'] in ['PA', 'PE']:\n",
    "        Happy.append(row['词语'])\n",
    "    if row['情感分类'] in ['PD', 'PH', 'PG', 'PB', 'PK']:\n",
    "        Good.append(row['词语']) \n",
    "    if row['情感分类'] in ['PC']:\n",
    "        Surprise.append(row['词语'])       \n",
    "    if row['情感分类'] in ['NB', 'NJ', 'NH', 'PF']:\n",
    "        Sad.append(row['词语'])\n",
    "    if row['情感分类'] in ['NI', 'NC', 'NG']:\n",
    "        Fear.append(row['词语'])\n",
    "    if row['情感分类'] in ['NE', 'ND', 'NN', 'NK', 'NL']:\n",
    "        Disgust.append(row['词语'])\n",
    "    if row['情感分类'] in ['NAU']:     #修改: 原NA算出来没结果\n",
    "        Anger.append(row['词语'])  \n",
    "\n",
    "#正负计算不是很准 自己可以制定规则       \n",
    "Positive = Happy + Good + Surprise\n",
    "Negative = Anger + Sad + Fear + Disgust\n",
    "print('情绪词语列表整理完成')  \n",
    "print(Anger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20f4bb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/nz/30kw5j2n66b7xzhkd6pm3jqw0000gn/T/jieba.cache\n",
      "Loading model cost 0.271 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------中文分词---------------------------------\n",
    "import jieba\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#添加使用者词典和停用词\n",
    "jieba.load_userdict(\"user_dict.txt\")              #自定义词典\n",
    "stop_list = pd.read_csv('stop_word.txt',\n",
    "                        engine='python',\n",
    "                        encoding='utf-8',\n",
    "                        delimiter=\"/n\",\n",
    "                        names=['t'])['t'].tolist()\n",
    "def txt_cut(juzi):\n",
    "    return [w for w in jieba.lcut(juzi) if w not in stop_list]     #可增加len(w)>1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae253bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length      0\n",
      "positive    0\n",
      "negative    0\n",
      "anger       0\n",
      "disgust     0\n",
      "fear        0\n",
      "sadness     0\n",
      "surprise    0\n",
      "good        0\n",
      "happy       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------------中文分词---------------------------------\n",
    "import jieba\n",
    "import time\n",
    "\n",
    "#添加自定义词典和停用词\n",
    "jieba.load_userdict(\"user_dict.txt\")\n",
    "\n",
    "stop_list = pd.read_csv('stop_word.txt',\n",
    "                        engine='python',\n",
    "                        encoding='utf-8',\n",
    "                        delimiter=\"/n\",\n",
    "                        names=['t'])\n",
    "#获取重命名t列的值\n",
    "stop_list = stop_list['t'].tolist()\n",
    "\n",
    "def txt_cut(juzi):\n",
    "    return [w for w in jieba.lcut(juzi) if w not in stop_list]     #可增加len(w)>1\n",
    "\n",
    "#---------------------------------------情感计算---------------------------------\n",
    "def emotion_caculate(text):\n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    anger = 0\n",
    "    disgust = 0\n",
    "    fear = 0\n",
    "    sad = 0\n",
    "    surprise = 0\n",
    "    good = 0\n",
    "    happy = 0\n",
    "    \n",
    "    wordlist = txt_cut(text)\n",
    "    #wordlist = jieba.lcut(text)\n",
    "    wordset = set(wordlist)\n",
    "    wordfreq = []\n",
    "    for word in wordset:\n",
    "        freq = wordlist.count(word)\n",
    "        if word in Positive:\n",
    "            positive+=freq\n",
    "        if word in Negative:\n",
    "            negative+=freq\n",
    "        if word in Anger:\n",
    "            anger+=freq  \n",
    "        if word in Disgust:\n",
    "            disgust+=freq\n",
    "        if word in Fear:\n",
    "            fear+=freq\n",
    "        if word in Sad:\n",
    "            sad+=freq\n",
    "        if word in Surprise:\n",
    "            surprise+=freq\n",
    "        if word in Good:\n",
    "            good+=freq\n",
    "        if word in Happy:\n",
    "            happy+=freq\n",
    "            \n",
    "    emotion_info = {\n",
    "        'length':len(wordlist),\n",
    "        'positive': positive,\n",
    "        'negative': negative,\n",
    "        'anger': anger,\n",
    "        'disgust': disgust,\n",
    "        'fear':fear,\n",
    "        'good':good,\n",
    "        'sadness':sad,\n",
    "        'surprise':surprise,\n",
    "        'happy':happy,\n",
    "        \n",
    "    }\n",
    "\n",
    "    indexs = ['length', 'positive', 'negative', 'anger', 'disgust','fear','sadness','surprise', 'good', 'happy']\n",
    "    return pd.Series(emotion_info, index=indexs)\n",
    "\n",
    "#测试\n",
    "text=\"\"\n",
    "\n",
    "res = emotion_caculate(text)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5b520b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#---------------------------------------情感计算---------------------------------\u001b[39;00m\n\u001b[1;32m      2\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()   \n\u001b[0;32m----> 3\u001b[0m emotion_df \u001b[38;5;241m=\u001b[39m weibo_df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(emotion_caculate)\n\u001b[1;32m      4\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(end\u001b[38;5;241m-\u001b[39mstart)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:4760\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4627\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4632\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4633\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4634\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4635\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4636\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4751\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4752\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4753\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[1;32m   4754\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4755\u001b[0m         func,\n\u001b[1;32m   4756\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[1;32m   4757\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[1;32m   4758\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   4759\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m-> 4760\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:1207\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:1287\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1287\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[1;32m   1288\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[1;32m   1289\u001b[0m )\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[1;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1818\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2920\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[12], line 31\u001b[0m, in \u001b[0;36memotion_caculate\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     28\u001b[0m good \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     29\u001b[0m happy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 31\u001b[0m wordlist \u001b[38;5;241m=\u001b[39m txt_cut(text)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#wordlist = jieba.lcut(text)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m wordset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(wordlist)\n",
      "Cell \u001b[0;32mIn[12], line 17\u001b[0m, in \u001b[0;36mtxt_cut\u001b[0;34m(juzi)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtxt_cut\u001b[39m(juzi):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m jieba\u001b[38;5;241m.\u001b[39mlcut(juzi) \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_list]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/jieba/__init__.py:357\u001b[0m, in \u001b[0;36mTokenizer.lcut\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlcut\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcut(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/jieba/__init__.py:300\u001b[0m, in \u001b[0;36mTokenizer.cut\u001b[0;34m(self, sentence, cut_all, HMM, use_paddle)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03mThe main function that segments an entire sentence that contains\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03mChinese characters into separated words.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;124;03m    - HMM: Whether to use the Hidden Markov Model.\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    299\u001b[0m is_paddle_installed \u001b[38;5;241m=\u001b[39m check_paddle_install[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_paddle_installed\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 300\u001b[0m sentence \u001b[38;5;241m=\u001b[39m strdecode(sentence)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_paddle \u001b[38;5;129;01mand\u001b[39;00m is_paddle_installed:\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;66;03m# if sentence is null, it will raise core exception in paddle.\u001b[39;00m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sentence \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sentence) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/jieba/_compat.py:79\u001b[0m, in \u001b[0;36mstrdecode\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sentence, text_type):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m         sentence \u001b[38;5;241m=\u001b[39m sentence\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m         sentence \u001b[38;5;241m=\u001b[39m sentence\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgbk\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "#---------------------------------------情感计算---------------------------------\n",
    "start = time.time()   \n",
    "emotion_df = weibo_df.iloc[:, 0].apply(emotion_caculate)\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "print(emotion_df.head())\n",
    "\n",
    "#输出结果\n",
    "output_df = pd.concat([weibo_df, emotion_df], axis=1)\n",
    "output_df.to_csv('花西子跌出榜单前10评论.csv_emotion.csv',encoding='utf_8_sig', index=False)\n",
    "print(output_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2cc698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c1e57d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee41832",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
